{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/CESARIUX2596/CS-Master/blob/master/Artificial_Vision_FInal_Proyect_Data_Augmentation_Aproach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzIJV_wYjoR1"
   },
   "source": [
    "# Guitar classification\n",
    "In this project we will create a model that classifies guitar by body type in realtime using the videofeed of a camera.\n",
    "The body types available to clasify are the following:\n",
    "\n",
    "\n",
    "*   Acoustic\n",
    "*   Double Cut\n",
    "*   Les Paul\n",
    "*   Telecaster\n",
    "*   Strat\n",
    "*   Ukulele\n",
    "\n",
    "\n",
    "In this Aproach we will use data augmentation to provide more training information to the Convolutional Neuronal Network while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eSGSgHkxjLXs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lhE57Abej_Lj",
    "outputId": "9dd355ce-32d1-4e4a-e3b2-7c127725dcd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Mount Google drive as a device\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jAfA-MmWkYUt",
    "outputId": "1c997594-f8d9-4a13-ba1a-c5a6b24e1d7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caos_guitar_dataset  Model1  tacos  TTY_dataset\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The folder structure contains the Train and Test directories\n",
    "Each diractory has subfolders for each class that the classificatior will accept\n",
    "'''\n",
    "# Mount the folder with the dataset\n",
    "os.chdir(\"/content/drive/My Drive/datasets/\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5wGE2o6rxE41"
   },
   "outputs": [],
   "source": [
    "base_directory =  'caos_guitar_dataset'\n",
    "train_directory = os.path.join(base_directory, 'Train')\n",
    "test_directory = os.path.join(base_directory, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "u2VL_CO1xhQ6"
   },
   "outputs": [],
   "source": [
    "# Training directories\n",
    "train_acoustic = os.path.join(train_directory, 'Acoustic')\n",
    "train_double_cut = os.path.join(train_directory, 'Double_Cut')\n",
    "train_les_paul = os.path.join(train_directory, 'Les_Paul')\n",
    "train_strat = os.path.join(train_directory, 'Strat')\n",
    "train_tele = os.path.join(train_directory, 'Telecaster')\n",
    "train_ukulele = os.path.join(train_directory, 'Ukulele')\n",
    "\n",
    "# Test directories\n",
    "test_acoustic = os.path.join(test_directory, 'Acoustic')\n",
    "test_double_cut = os.path.join(test_directory, 'Double_Cut')\n",
    "test_les_paul = os.path.join(test_directory, 'Les_Paul')\n",
    "test_strat = os.path.join(test_directory, 'Strat')\n",
    "test_tele = os.path.join(test_directory, 'Telecaster')\n",
    "test_ukulele = os.path.join(test_directory, 'Ukulele')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2dpHhp6g1rsj"
   },
   "outputs": [],
   "source": [
    "# Simple Convolutional Neuronal Network \n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150,3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(256, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(6, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oZTRehEpBFX1"
   },
   "outputs": [],
   "source": [
    "# LEARNING_RATE = 1e-4\n",
    "# We use this lost function because the labels are probided as One Hot Encoding\n",
    "LOSS = 'CategoricalCrossentropy' \n",
    "model.compile(loss=LOSS,\n",
    "              optimizer=Adagrad(lr=0.01, epsilon=1e-6),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EH2mchoQHJkq",
    "outputId": "9dfd711d-e270-4b15-e719-efd6fe4b2962"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4320 images belonging to 6 classes.\n",
      "Found 1080 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create Data Generator for the image Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "      \n",
    "      rescale=1./255,\n",
    "      rotation_range=45,\n",
    "      width_shift_range=0.20,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.20,\n",
    "      zoom_range=0.3,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='reflect')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_directory,  # This is the source directory for training images\n",
    "        color_mode='rgb',\n",
    "        target_size=(150, 150),  # All images will be resized to 128x128\n",
    "        batch_size=40,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='categorical')\n",
    "\n",
    "# Flow validation images in batches of 20 using test_datagen generator\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        test_directory,\n",
    "        color_mode='rgb',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='categorical')\n",
    "\n",
    "# history = model.fit(\n",
    "#       train_generator,\n",
    "#       steps_per_epoch=108,  # 2000 images = batch_size * steps\n",
    "#       epochs=150,\n",
    "#       validation_data=validation_generator,\n",
    "#       validation_steps=54,  # 1000 images = batch_size * steps\n",
    "#       verbose=2)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uQp0DPDFgCdz",
    "outputId": "52b220d9-db2d-433b-ab24-f38e620ec36a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Model1/assets\n"
     ]
    }
   ],
   "source": [
    "# model.save('Model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2D307Pp9Ctj3"
   },
   "outputs": [],
   "source": [
    "reconstructed_model = keras.models.load_model(\"Model1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5uGs2_pgEp0",
    "outputId": "32a9132f-2bb1-4d41-c7b2-73a73d8c4420"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 - 2038s - loss: 1.7103 - accuracy: 0.3097 - val_loss: 1.3169 - val_accuracy: 0.4556\n",
      "Epoch 2/60\n",
      "108/108 - 174s - loss: 1.1834 - accuracy: 0.5470 - val_loss: 1.0405 - val_accuracy: 0.5991\n",
      "Epoch 3/60\n",
      "108/108 - 168s - loss: 1.0459 - accuracy: 0.5933 - val_loss: 1.0567 - val_accuracy: 0.6185\n",
      "Epoch 4/60\n",
      "108/108 - 169s - loss: 0.9974 - accuracy: 0.6211 - val_loss: 1.0349 - val_accuracy: 0.6222\n",
      "Epoch 5/60\n",
      "108/108 - 168s - loss: 0.9939 - accuracy: 0.6150 - val_loss: 1.0135 - val_accuracy: 0.6269\n",
      "Epoch 6/60\n",
      "108/108 - 167s - loss: 0.9418 - accuracy: 0.6419 - val_loss: 1.2207 - val_accuracy: 0.5454\n",
      "Epoch 7/60\n",
      "108/108 - 167s - loss: 0.9620 - accuracy: 0.6294 - val_loss: 0.9493 - val_accuracy: 0.6389\n",
      "Epoch 8/60\n",
      "108/108 - 165s - loss: 0.9291 - accuracy: 0.6400 - val_loss: 1.0621 - val_accuracy: 0.6028\n",
      "Epoch 9/60\n",
      "108/108 - 167s - loss: 0.9330 - accuracy: 0.6421 - val_loss: 0.9625 - val_accuracy: 0.6463\n",
      "Epoch 10/60\n",
      "108/108 - 168s - loss: 0.9383 - accuracy: 0.6370 - val_loss: 0.9947 - val_accuracy: 0.6361\n",
      "Epoch 11/60\n",
      "108/108 - 168s - loss: 0.9120 - accuracy: 0.6537 - val_loss: 1.0422 - val_accuracy: 0.6176\n",
      "Epoch 12/60\n",
      "108/108 - 168s - loss: 0.9053 - accuracy: 0.6481 - val_loss: 1.0242 - val_accuracy: 0.6231\n",
      "Epoch 13/60\n",
      "108/108 - 167s - loss: 0.8670 - accuracy: 0.6741 - val_loss: 1.1071 - val_accuracy: 0.6213\n",
      "Epoch 14/60\n",
      "108/108 - 169s - loss: 0.8730 - accuracy: 0.6713 - val_loss: 0.9717 - val_accuracy: 0.6481\n",
      "Epoch 15/60\n",
      "108/108 - 169s - loss: 0.8746 - accuracy: 0.6706 - val_loss: 1.0764 - val_accuracy: 0.6111\n",
      "Epoch 16/60\n",
      "108/108 - 168s - loss: 0.8791 - accuracy: 0.6641 - val_loss: 1.0072 - val_accuracy: 0.6398\n",
      "Epoch 17/60\n",
      "108/108 - 168s - loss: 0.8768 - accuracy: 0.6718 - val_loss: 0.9800 - val_accuracy: 0.6463\n",
      "Epoch 18/60\n",
      "108/108 - 169s - loss: 0.8577 - accuracy: 0.6750 - val_loss: 0.9835 - val_accuracy: 0.6435\n",
      "Epoch 19/60\n",
      "108/108 - 169s - loss: 0.8602 - accuracy: 0.6762 - val_loss: 0.9836 - val_accuracy: 0.6472\n",
      "Epoch 20/60\n",
      "108/108 - 168s - loss: 0.8452 - accuracy: 0.6868 - val_loss: 0.9410 - val_accuracy: 0.6491\n",
      "Epoch 21/60\n",
      "108/108 - 168s - loss: 0.8529 - accuracy: 0.6792 - val_loss: 1.0125 - val_accuracy: 0.6296\n",
      "Epoch 22/60\n",
      "108/108 - 168s - loss: 0.8373 - accuracy: 0.6817 - val_loss: 0.9504 - val_accuracy: 0.6565\n",
      "Epoch 23/60\n",
      "108/108 - 167s - loss: 0.8258 - accuracy: 0.6942 - val_loss: 0.9807 - val_accuracy: 0.6602\n",
      "Epoch 24/60\n",
      "108/108 - 169s - loss: 0.8084 - accuracy: 0.6984 - val_loss: 1.0314 - val_accuracy: 0.6315\n",
      "Epoch 25/60\n",
      "108/108 - 169s - loss: 0.8035 - accuracy: 0.6972 - val_loss: 1.0974 - val_accuracy: 0.6074\n",
      "Epoch 26/60\n",
      "108/108 - 168s - loss: 0.8097 - accuracy: 0.6935 - val_loss: 1.0840 - val_accuracy: 0.6157\n",
      "Epoch 27/60\n",
      "108/108 - 169s - loss: 0.8209 - accuracy: 0.6938 - val_loss: 0.9771 - val_accuracy: 0.6546\n",
      "Epoch 28/60\n",
      "108/108 - 168s - loss: 0.8011 - accuracy: 0.7035 - val_loss: 1.0379 - val_accuracy: 0.6556\n",
      "Epoch 29/60\n",
      "108/108 - 168s - loss: 0.8068 - accuracy: 0.6958 - val_loss: 1.1116 - val_accuracy: 0.6306\n",
      "Epoch 30/60\n",
      "108/108 - 168s - loss: 0.7882 - accuracy: 0.6979 - val_loss: 1.2269 - val_accuracy: 0.6111\n",
      "Epoch 31/60\n",
      "108/108 - 170s - loss: 0.7820 - accuracy: 0.7035 - val_loss: 1.0290 - val_accuracy: 0.6500\n",
      "Epoch 32/60\n",
      "108/108 - 169s - loss: 0.7789 - accuracy: 0.6956 - val_loss: 1.0384 - val_accuracy: 0.6426\n",
      "Epoch 33/60\n",
      "108/108 - 170s - loss: 0.7748 - accuracy: 0.7113 - val_loss: 0.9608 - val_accuracy: 0.6620\n",
      "Epoch 34/60\n",
      "108/108 - 170s - loss: 0.7721 - accuracy: 0.7100 - val_loss: 0.9435 - val_accuracy: 0.6750\n",
      "Epoch 35/60\n",
      "108/108 - 170s - loss: 0.7692 - accuracy: 0.7146 - val_loss: 1.0167 - val_accuracy: 0.6611\n",
      "Epoch 36/60\n",
      "108/108 - 170s - loss: 0.7618 - accuracy: 0.7176 - val_loss: 0.9506 - val_accuracy: 0.6546\n",
      "Epoch 37/60\n",
      "108/108 - 171s - loss: 0.7773 - accuracy: 0.7019 - val_loss: 0.9894 - val_accuracy: 0.6556\n",
      "Epoch 38/60\n",
      "108/108 - 171s - loss: 0.7739 - accuracy: 0.7081 - val_loss: 1.0147 - val_accuracy: 0.6694\n",
      "Epoch 39/60\n",
      "108/108 - 172s - loss: 0.7595 - accuracy: 0.7146 - val_loss: 0.9678 - val_accuracy: 0.6519\n",
      "Epoch 40/60\n",
      "108/108 - 170s - loss: 0.7500 - accuracy: 0.7190 - val_loss: 1.0086 - val_accuracy: 0.6630\n",
      "Epoch 41/60\n",
      "108/108 - 167s - loss: 0.7534 - accuracy: 0.7139 - val_loss: 0.9765 - val_accuracy: 0.6593\n",
      "Epoch 42/60\n",
      "108/108 - 168s - loss: 0.7235 - accuracy: 0.7208 - val_loss: 0.9988 - val_accuracy: 0.6713\n",
      "Epoch 43/60\n",
      "108/108 - 167s - loss: 0.7500 - accuracy: 0.7160 - val_loss: 0.9752 - val_accuracy: 0.6704\n",
      "Epoch 44/60\n",
      "108/108 - 168s - loss: 0.7338 - accuracy: 0.7319 - val_loss: 0.9742 - val_accuracy: 0.6731\n",
      "Epoch 45/60\n",
      "108/108 - 168s - loss: 0.7399 - accuracy: 0.7141 - val_loss: 0.9319 - val_accuracy: 0.6880\n",
      "Epoch 46/60\n",
      "108/108 - 170s - loss: 0.7302 - accuracy: 0.7238 - val_loss: 0.9462 - val_accuracy: 0.6731\n",
      "Epoch 47/60\n",
      "108/108 - 176s - loss: 0.7267 - accuracy: 0.7255 - val_loss: 0.9662 - val_accuracy: 0.6667\n",
      "Epoch 48/60\n",
      "108/108 - 173s - loss: 0.7096 - accuracy: 0.7336 - val_loss: 0.9616 - val_accuracy: 0.6731\n",
      "Epoch 49/60\n",
      "108/108 - 171s - loss: 0.7289 - accuracy: 0.7317 - val_loss: 0.9645 - val_accuracy: 0.6741\n",
      "Epoch 50/60\n",
      "108/108 - 172s - loss: 0.7000 - accuracy: 0.7338 - val_loss: 1.0242 - val_accuracy: 0.6509\n",
      "Epoch 51/60\n",
      "108/108 - 170s - loss: 0.7035 - accuracy: 0.7373 - val_loss: 1.0285 - val_accuracy: 0.6815\n",
      "Epoch 52/60\n",
      "108/108 - 172s - loss: 0.7031 - accuracy: 0.7373 - val_loss: 1.0404 - val_accuracy: 0.6713\n",
      "Epoch 53/60\n",
      "108/108 - 172s - loss: 0.6918 - accuracy: 0.7380 - val_loss: 0.9915 - val_accuracy: 0.6667\n",
      "Epoch 54/60\n",
      "108/108 - 169s - loss: 0.6801 - accuracy: 0.7491 - val_loss: 0.9704 - val_accuracy: 0.6898\n",
      "Epoch 55/60\n",
      "108/108 - 171s - loss: 0.6896 - accuracy: 0.7400 - val_loss: 1.0070 - val_accuracy: 0.6741\n",
      "Epoch 56/60\n",
      "108/108 - 169s - loss: 0.6785 - accuracy: 0.7491 - val_loss: 0.9795 - val_accuracy: 0.6722\n",
      "Epoch 57/60\n",
      "108/108 - 171s - loss: 0.6933 - accuracy: 0.7373 - val_loss: 1.0622 - val_accuracy: 0.6602\n",
      "Epoch 58/60\n",
      "108/108 - 170s - loss: 0.6738 - accuracy: 0.7461 - val_loss: 0.9523 - val_accuracy: 0.6796\n",
      "Epoch 59/60\n",
      "108/108 - 170s - loss: 0.6667 - accuracy: 0.7567 - val_loss: 0.9951 - val_accuracy: 0.6796\n",
      "Epoch 60/60\n",
      "108/108 - 169s - loss: 0.6727 - accuracy: 0.7519 - val_loss: 0.9849 - val_accuracy: 0.6694\n"
     ]
    }
   ],
   "source": [
    "# Continue training model to get 210 epochs\n",
    "history = reconstructed_model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=108,  # 2000 images = batch_size * steps\n",
    "      epochs=60,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=54,  # 1000 images = batch_size * steps\n",
    "      verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_27IyioXCc5m",
    "outputId": "d804d111-f588-458d-f7e4-799e9ed235a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Model1_Extended_210Epochs/assets\n"
     ]
    }
   ],
   "source": [
    "reconstructed_model.save('Model1_Extended_210Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-9UlE5y5axjL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOfKMmX5/l6AjCEYztNz0/F",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Artificial_Vision_FInal_Proyect_Data_Augmentation_Aproach.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
